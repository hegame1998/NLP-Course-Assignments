{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPupiQIQ7XwRCbu8ygliOp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegame1998/NLP-Course-Assignments/blob/main/NLP%20Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will do this approach in this code:\n",
        "\n",
        "\n",
        "* **Loads two documents:** one to summarize, one as style reference.\n",
        "\n",
        "* **Estimates token length** using word count (proxy for 4000-token limit).\n",
        "\n",
        "* **Performs chunk-based summarization** using TextRank-style TF-IDF cosine similarity.\n",
        "\n",
        "* **If the summary is too large**, it recursively shrinks it.\n",
        "\n",
        "* **Saves the summaries.**\n",
        "\n",
        "* **Prints a query prompt** to generate a style-following summary."
      ],
      "metadata": {
        "id": "oEUtBIJj_XKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Collection"
      ],
      "metadata": {
        "id": "-bYgasEGDHiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where I load my input and style reference documents.<br> Read two input text files (T1: style source, T2: text to summarize).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7LXniapmDbse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection\n",
        "def load_documents(style_path, target_path):\n",
        "    with open(style_path, 'r', encoding='utf-8') as f:\n",
        "        style_text = f.read()\n",
        "    with open(target_path, 'r', encoding='utf-8') as f:\n",
        "        target_text = f.read()\n",
        "    return style_text, target_text"
      ],
      "metadata": {
        "id": "Q6dF7HNjEWI5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "lrSiUDQRDSM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean and tokenize the text.\n",
        "\n",
        "* Tokenize T1 and T2.\n",
        "\n",
        "* Count token lengths.\n",
        "\n",
        "* Define target lengths proportionally."
      ],
      "metadata": {
        "id": "Rgrd9ocpDhRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def preprocess(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    return sentences, words\n",
        "\n",
        "def get_token_count(words):\n",
        "    return len(words)\n",
        "\n",
        "def split_into_chunks(sentences, max_tokens):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    token_count = 0\n",
        "    for sent in sentences:\n",
        "        tokens = word_tokenize(sent)\n",
        "        if token_count + len(tokens) > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sent]\n",
        "            token_count = len(tokens)\n",
        "        else:\n",
        "            current_chunk.append(sent)\n",
        "            token_count += len(tokens)\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "IPoeqT2oDVjO",
        "outputId": "001086e7-8a66-4c46-862d-ee18173225eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Style Feature Extraction"
      ],
      "metadata": {
        "id": "1xV4brfYDY3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use TF-IDF and cosine similarity to rank sentences for summarization.\n",
        "\n",
        "* Extract stylistic features from T1 (sentence length, punctuation usage, common POS tags).\n",
        "\n",
        "* Optionally, compute average sentence length and POS tag distribution.\n"
      ],
      "metadata": {
        "id": "Zpf3mDt5Dkgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def extract_style_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    pos_counts = Counter(tag for word, tag in tagged)\n",
        "    sentences = sent_tokenize(text)\n",
        "    avg_len = sum(len(word_tokenize(s)) for s in sentences) / len(sentences)\n",
        "    return {'pos_distribution': pos_counts, 'avg_sentence_length': avg_len}"
      ],
      "metadata": {
        "id": "ElxMj4FeDnJz",
        "outputId": "224522b7-eca8-47fb-ce9b-fd3627edabac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training (Summarization Logic)"
      ],
      "metadata": {
        "id": "_jhXgByhDsYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteratively summarize large texts to fit the context window (e.g., 4000 tokens).\n",
        "\n",
        "* Weâ€™ll implement an extractive summarization method using scoring (TF-IDF or frequency-based).\n",
        "\n",
        "* No external model training is required.\n",
        "\n",
        "* For style adaptation, re-rank or rewrite based on stylistic features from T1."
      ],
      "metadata": {
        "id": "TyephVpGDvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization Logic\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "def score_sentences(sentences, style_features):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    scores = {}\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent.lower())\n",
        "        words = [w for w in words if w not in stop_words and w not in string.punctuation]\n",
        "        score = len(words)\n",
        "        if abs(len(words) - style_features['avg_sentence_length']) < 5:\n",
        "            score += 2  # stylistic bonus\n",
        "        scores[sent] = score\n",
        "    return scores\n",
        "\n",
        "def summarize_chunk(chunk, style_features, target_sentences=5):\n",
        "    sentences = sent_tokenize(chunk)\n",
        "    scores = score_sentences(sentences, style_features)\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected = [sent for sent, score in ranked[:target_sentences]]\n",
        "    return ' '.join(selected)\n",
        "\n"
      ],
      "metadata": {
        "id": "pH9dTZGDDuma",
        "outputId": "fb1cee04-7576-450b-eeed-54ec81a5a355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Function"
      ],
      "metadata": {
        "id": "AYddWpftD1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output and save summaries, simulate a style-following prompt.\n",
        "\n",
        "* Manual or ROUGE-based metrics (if available).\n",
        "\n",
        "* Summary length check vs context window."
      ],
      "metadata": {
        "id": "v_NqhIKzD3K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "def check_summary_length(summary, token_limit=4000):\n",
        "    words = word_tokenize(summary)\n",
        "    return len(words) <= token_limit"
      ],
      "metadata": {
        "id": "tMBYUPYPD6pb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Pipeline"
      ],
      "metadata": {
        "id": "y2WTpIhXD-le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tie everything together.\n",
        "\n",
        "* Orchestrates all components.\n",
        "\n",
        "* Repeats summarization until the result fits within the token limit.\n"
      ],
      "metadata": {
        "id": "wqOZD7IKEBKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "def hierarchical_summarize(style_text, target_text, token_limit=4000):\n",
        "    _, style_words = preprocess(style_text)\n",
        "    style_features = extract_style_features(style_text)\n",
        "\n",
        "    target_sentences, target_words = preprocess(target_text)\n",
        "    if len(target_words) <= token_limit:\n",
        "        return summarize_chunk(target_text, style_features)\n",
        "\n",
        "    chunks = split_into_chunks(target_sentences, token_limit)\n",
        "    summaries = [summarize_chunk(chunk, style_features) for chunk in chunks]\n",
        "\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Recursively summarize if still too long\n",
        "    while not check_summary_length(final_summary, token_limit):\n",
        "        final_summary = summarize_chunk(final_summary, style_features)\n",
        "\n",
        "    return final_summary\n"
      ],
      "metadata": {
        "id": "HhacIpOsEC1s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the Code"
      ],
      "metadata": {
        "id": "BPpKgm2zyLwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_text, target_text = load_documents('style.txt', 'to_summarize.txt')\n",
        "final_summary = hierarchical_summarize(style_text, target_text)"
      ],
      "metadata": {
        "id": "pKoSFLx4yKGV",
        "outputId": "ccd09bd6-8b13-480b-dd92-f447ef332236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'style.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-841238605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'style.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to_summarize.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchical_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3661412786.py\u001b[0m in \u001b[0;36mload_documents\u001b[0;34m(style_path, target_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data Collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mstyle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'style.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Output"
      ],
      "metadata": {
        "id": "2roVwsPG1V8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_summary)\n",
        "\n",
        "print(\"âœ… Summary generated and saved as summary.txt\")\n",
        "files.download(\"summary.txt\")"
      ],
      "metadata": {
        "id": "DDepjJeQ1TuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}